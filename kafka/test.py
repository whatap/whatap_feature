doc = """[{\"interval_sec\":60,\"query\":\"CATEGORY telegraf_kafka_controller\\nTAGLOAD {backward:true}\\nSELECT [ 'pcode', 'time', 'oid', 'Value', 'type', 'name' ]\\nSKIP 3\\nFILTER { key: \\\"Value\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"KafkaController\\\" }\\nFILTER { key: \\\"name\\\", value: \\\"OfflinePartitionsCount\\\" }\\nGROUP { timegroup:10s, pk:pcode, merge:[Value]}\\nUPDATE { key:[Value], value:sum, keep: \\\"time\\\" }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"Value\\\"]\\n\",\"rule\":\"Value > 0\",\"event_level\":30,\"event_title\":\"Offline Partitions Detected\",\"time_trim\":5000,\"event_message\":\"There are currently ${Value} offline partitions. Immediate investigation and resolution are required to maintain cluster integrity.\",\"enabled\":true,\"time_period\":30000,\"stateful\":false,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714379700000,\\\"etime\\\":1714380300000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"2febe921-21fc-4019-9d5e-d29d7b1443ba\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"},{\"interval_sec\":60,\"query\":\"CATEGORY telegraf_kafka_server\\nTAGLOAD {backward:true}\\nSELECT [ 'pcode', 'time', 'oid', 'Value', 'type', 'name' ]\\nSKIP 3\\nFILTER { key: \\\"Value\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"ReplicaManager\\\" }\\nFILTER { key: \\\"name\\\", value: \\\"UnderReplicatedPartitions\\\" }\\nFIRST-ONLY { key: \\\"oid\\\" }\\nGROUP { timeunit:10s, pk:pcode, merge:[Value] }\\nUPDATE { key:[Value], value:sum }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"Value\\\"]\\n\",\"rule\":\"Value > 0\",\"event_level\":30,\"event_title\":\"Under-Replicated Partitions Found\",\"time_trim\":5000,\"event_message\":\"There are ${Value} under-replicated partitions in the cluster. This may risk data loss and reduced fault tolerance.\",\"enabled\":true,\"time_period\":60000,\"stateful\":true,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714380660000,\\\"etime\\\":1714381260000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"4ae8ca05-6e0e-4c6e-bafc-2beb858f5448\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"},{\"interval_sec\":60,\"query\":\"CATEGORY telegraf_kafka_controller\\nTAGLOAD {backward:true}\\nSELECT [ 'pcode','oid', 'time', 'oid', 'Value', 'type', 'name' ]\\nFILTER { key: \\\"Value\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"KafkaController\\\" }\\nFILTER { key: \\\"name\\\", value: \\\"ActiveControllerCount\\\" }\\nGROUP { timegroup:\\\"10s\\\", pk:\\\"pcode\\\", merge:[Value], keep:\\\"time\\\" }\\nUPDATE { key:[Value], value:sum }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"Value\\\"]\\n\",\"rule\":\"Value != 1\",\"event_level\":30,\"event_title\":\"Controller Count Issue Detected\",\"time_trim\":5000,\"event_message\":\"The number of active controllers is ${Value}. There should be exactly one active controller in the cluster.\",\"enabled\":true,\"time_period\":60000,\"stateful\":false,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714391460000,\\\"etime\\\":1714392060000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"54b28e6a-eb81-4224-b0c4-a7362fdc9b03\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"},{\"interval_sec\":60,\"query\":\"CATEGORY telegraf_kafka_controller\\nTAGLOAD {backward:true}\\nSELECT [ 'pcode', 'time', 'oid', 'OneMinuteRate', 'type', 'name' ]\\nSKIP 3\\nFILTER { key: \\\"OneMinuteRate\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"ControllerStats\\\" }\\nFILTER { key: \\\"name\\\", value: \\\"UncleanLeaderElectionsPerSec\\\" }\\nFIRST-ONLY { key: \\\"oid\\\" }\\nGROUP { timeunit:10s, pk:pcode, merge:[OneMinuteRate] }\\nUPDATE { key:[OneMinuteRate], value:sum }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"Value\\\"]\\n\",\"rule\":\"Value > 0\",\"event_level\":30,\"event_title\":\"Unclean Leader Elections Occurring\",\"time_trim\":5000,\"event_message\":\" There have been ${Value} unclean leader elections. This can lead to potential data loss.\",\"enabled\":true,\"time_period\":60000,\"stateful\":false,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714391940000,\\\"etime\\\":1714392540000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"c35b6180-d34d-4912-b831-fad323b7f83e\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"},{\"interval_sec\":60,\"query\":\"CATEGORY {\\\"telegraf_kafka_server\\\":6h, \\\"telegraf_kafka_server{m5}\\\":3d, \\\"telegraf_kafka_server{h1}\\\":unlimit } \\nTAGLOAD\\nSELECT [ 'pcode', 'time', 'oid', 'OneMinuteRate', 'pname', 'type', 'name' ]\\nSKIP 3\\nFILTER { key: \\\"OneMinuteRate\\\", exist: true }\\nFILTER { key: \\\"pname\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"ReplicaManager\\\"}\\nFILTER { key: \\\"name\\\", value: \\\"IsrShrinksPerSec\\\"}\\nGROUP { timegroup:\\\"10s\\\", merge: ['OneMinuteRate'], pk: \\\"pcode\\\", last: time}\\nUPDATE { key: ['OneMinuteRate'], value: sum }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"OneMinuteRate\\\"]\\n\",\"rule\":\"FifteenMinuteRate*10 < OneMinuteRate\",\"event_level\":30,\"event_title\":\"High ISR Expansion Rate\",\"time_trim\":5000,\"event_message\":\"The ISR expansion rate has increased unexpectedly to ${OneMinuteRate} per minute. Monitor for potential stability issues.\",\"enabled\":true,\"time_period\":60000,\"stateful\":false,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714392060000,\\\"etime\\\":1714392660000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"2f0ff006-ced5-448d-88bf-a0debcb3ae89\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"},{\"interval_sec\":60,\"query\":\"OIDSET { oid:$oid, okind:$okind, onode:$onode }\\nCATEGORY {\\\"telegraf_kafka_server\\\":6h, \\\"telegraf_kafka_server{m5}\\\":3d, \\\"telegraf_kafka_server{h1}\\\":unlimit } \\nTAGLOAD\\nSELECT [ 'pcode', 'time', 'oid', 'OneMinuteRate', 'pname', 'type', 'name' ]\\nSKIP 3\\nFILTER { key: \\\"OneMinuteRate\\\", exist: true }\\nFILTER { key: \\\"pname\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"ReplicaManager\\\"}\\nFILTER { key: \\\"name\\\", value: \\\"IsrShrinksPerSec\\\"}\\nGROUP { timeunit: 10s, merge: ['OneMinuteRate'], pk:pcode, last: time}\\nUPDATE { key: \\\"OneMinuteRate\\\", value: sum }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"OneMinuteRate\\\"]\\n\",\"rule\":\"FifteenMinuteRate * 10 < OneMinuteRate\",\"event_level\":30,\"event_title\":\"Increased ISR Shrink Rate\",\"time_trim\":5000,\"event_message\":\"The ISR shrink rate is currently ${OneMinuteRate} per minute. This could indicate synchronization problems among replicas.\",\"enabled\":true,\"time_period\":60000,\"stateful\":false,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714392660000,\\\"etime\\\":1714393260000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"6d797975-86a6-4ab8-ab93-ce8ba0d04ed6\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"},{\"interval_sec\":60,\"query\":\"CATEGORY {\\\"telegraf_kafka_server\\\":6h, \\\"telegraf_kafka_server{m5}\\\":3d, \\\"telegraf_kafka_server{h1}\\\":unlimit } \\nTAGLOAD\\nSELECT [ 'pcode', 'time', 'oid', 'OneMinuteRate', 'oname', 'type', 'name' ]\\nSKIP 3\\nFILTER { key: \\\"OneMinuteRate\\\", exist: true }\\nFILTER { key: \\\"oname\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"SessionExpireListener\\\"}\\nFILTER { key: \\\"name\\\", value: \\\"ZooKeeperExpiresPerSec\\\"}\\nGROUP { timeunit: 10s, merge: ['OneMinuteRate'], pk: pcode, last: time }\\nUPDATE { key: ['OneMinuteRate'], value: sum }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"OneMinuteRate\\\"]\\n\",\"rule\":\"FifteenMinuteRate*10 < OneMinuteRate\",\"event_level\":30,\"event_title\":\"Zookeeper Connection Issues\",\"time_trim\":5000,\"event_message\":\"Issues with Zookeeper synchronization have been detected. This may affect cluster stability and performance.\",\"enabled\":true,\"time_period\":60000,\"stateful\":false,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714393020000,\\\"etime\\\":1714393620000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"2165e90e-9e8b-49ee-969d-13b76585d625\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"},{\"interval_sec\":60,\"query\":\"CATEGORY telegraf_kafka_server\\nTAGLOAD {backward:true}\\nSELECT [ 'pcode', 'oid', 'time', 'oid', 'Value', 'type', 'name' ]\\nFILTER { key: \\\"Value\\\", exist: true }\\nFILTER { key: \\\"type\\\", value: \\\"ReplicaManager\\\" }\\nFILTER { key: \\\"name\\\", value: \\\"LeaderCount\\\" }\\nGROUP {timegroup:\\\"10s\\\", pk:\\\"pcode\\\", merge:[Value], keep:\\\"time\\\" }\\nUPDATE { key:[Value], value:count }\\nSKIP 1\\nLIMIT 1\\nSELECT [\\\"time\\\",\\\"Value\\\"]\\n\",\"rule\":\"Value < 3\",\"event_level\":30,\"event_title\":\"Broker Count Below Threshold\",\"time_trim\":5000,\"event_message\":\"The current broker count is ${Value}. This is below the required threshold for normal operations and may affect cluster performance.\",\"enabled\":true,\"time_period\":60000,\"stateful\":false,\"silent_sec\":300,\"desc\":\"{\\\"widgetType\\\":\\\"EXPERT\\\",\\\"supports\\\":[],\\\"isExpertEditable\\\":false,\\\"flexEventWidget\\\":{\\\"widgetType\\\":\\\"FLEX_EVENT\\\",\\\"id\\\":\\\"composite_metrics_widget\\\",\\\"requestApi\\\":\\\"LAST\\\",\\\"option\\\":{\\\"stime\\\":1714393200000,\\\"etime\\\":1714393800000,\\\"liveUpdateIntervalSec\\\":0,\\\"globalTime\\\":false,\\\"chart\\\":\\\"SERIES\\\",\\\"pcodes\\\":[2529],\\\"flex_event\\\":{\\\"category\\\":\\\"server_base\\\",\\\"tagPksGroup\\\":{\\\"pks\\\":[\\\"oid\\\"],\\\"timeunit\\\":5000},\\\"fieldsWithMerges\\\":[{\\\"key\\\":\\\"cpu\\\",\\\"timeMerge\\\":\\\"AVG\\\",\\\"objectMerge\\\":\\\"AVG\\\",\\\"unit\\\":\\\"PERCENT\\\",\\\"_label\\\":\\\"8ba23d92-b4e0-4836-abe6-51cf31944655\\\"}]},\\\"chartAside\\\":{\\\"chart\\\":\\\"TABLE\\\"},\\\"timeAction\\\":\\\"CUSTOM\\\"},\\\"metrics\\\":[{\\\"mql\\\":\\\"INJECT timepast\\\\nHEADER { \\\\\\\"cpu$\\\\\\\":\\\\\\\"PERCENT\\\\\\\" }\\\\nOIDSET { oid:$oid, okind:$okind, onode:$onode }\\\\nCATEGORY \\\\\\\"server_base\\\\\\\"\\\\nTAGLOAD\\\\nINJECT default\\\\nUPDATE {key: cpu, value: avg}\\\\nGROUP {pk:[oid], timeunit: 5000}\\\\nUPDATE {key: cpu, value: avg}\\\\nCREATE {key: _id_, expr:\\\\\\\"oid\\\\\\\"}\\\\nCREATE {key: _name_, expr:\\\\\\\"oid\\\\\\\"} \\\\nSELECT [_name_, _id_, time, oid, cpu]\\\\n\\\"}]},\\\"pcode\\\":2529}\"}]"""


import json
rules = json.loads(doc)

from pprint import pprint
for r in rules:
    pprint(r)

f = open('test.json', 'w')
f.write(json.dumps(rules, indent=2))
f.close()